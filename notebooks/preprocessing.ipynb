{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mediapipe import solutions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from video_converter import Video2DataFrame\n",
    "from custom_pose_landmarks import CustomPoseLandmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected values of pose landmarks corresponding to PoseLandmark class from MediaPipe library\n",
    "values = [0, 11, 12, 13, 14, 15, 16, 19, 20, 23, 24, 25, 26, 27, 28, 31, 32]\n",
    "\n",
    "# Custom pose landmark names and their connections\n",
    "landmarks = {\n",
    "    'THORAX': ['NOSE'],\n",
    "    'PELVIS': ['LEFT_HIP', 'RIGHT_HIP'],}\n",
    "\n",
    "# MediaPipe solutions\n",
    "mp_drawing = solutions.drawing_utils\n",
    "mp_pose = solutions.pose\n",
    "\n",
    "custom_pose = CustomPoseLandmark(mp_pose, values, landmarks)\n",
    "conv = Video2DataFrame(mp_pose, mp_drawing, custom_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "main_path = '../data/raw/'\n",
    "squat_path = os.path.join(main_path, 'squat')\n",
    "personal_data_path = os.path.join(main_path, 'PersonalData.xlsx')\n",
    "\n",
    "# Read personal data from excel\n",
    "personal_data = pd.read_excel(personal_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 001_01_05_01_040_1_C file to dataframe...\n",
      "Converting 001_01_05_01_040_1_L file to dataframe...\n",
      "Converting 001_01_05_02_040_1_C file to dataframe...\n",
      "Converting 001_01_05_02_040_1_L file to dataframe...\n",
      "Converting 001_01_05_03_040_1_C file to dataframe...\n",
      "Converting 001_01_05_03_040_1_L file to dataframe...\n",
      "Converting 001_01_05_03_040_1_R file to dataframe...\n",
      "Converting 001_01_05_04_040_1_C file to dataframe...\n",
      "Converting 001_01_05_04_040_1_L file to dataframe...\n",
      "Converting 001_01_05_04_040_1_R file to dataframe...\n",
      "Converting 001_01_05_05_040_1_C file to dataframe...\n",
      "Converting 001_01_05_05_040_1_L file to dataframe...\n",
      "Converting 001_01_05_05_040_1_R file to dataframe...\n",
      "Converting 001_02_03_01_070_1_C file to dataframe...\n",
      "Converting 001_02_03_01_070_1_L file to dataframe...\n",
      "Converting 001_02_03_01_070_1_R file to dataframe...\n",
      "Converting 001_02_03_02_070_1_C file to dataframe...\n",
      "Converting 001_02_03_02_070_1_L file to dataframe...\n",
      "Converting 001_02_03_02_070_1_R file to dataframe...\n",
      "Converting 001_02_03_03_070_1_C file to dataframe...\n",
      "Converting 001_02_03_03_070_1_L file to dataframe...\n",
      "Converting 001_02_03_03_070_1_R file to dataframe...\n",
      "Converting 001_03_01_01_090_1_C file to dataframe...\n"
     ]
    }
   ],
   "source": [
    "# Convert videos to dataframe\n",
    "df = conv.get_dataframe(\n",
    "    squat_path,\n",
    "    detection=0.9,\n",
    "    tracking=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('allsquats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('allsquats.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge personal and video data\n",
    "data = pd.merge(df, personal_data, on='Id')\n",
    "\n",
    "# Calculate the maximum load that was passed\n",
    "max_load = data.loc[data['Lifted'] == 1, ['Id', 'Load']].groupby(by='Id', as_index=False).max()\n",
    "max_load = max_load.rename(columns={'Load': 'MaxLoad'})\n",
    "data = pd.merge(data, max_load, on='Id')\n",
    "\n",
    "# Calculate what percentage of the maximum load is the current load\n",
    "data['PercentageMaxLoad'] = data['Load'] / data['MaxLoad']\n",
    "\n",
    "del data['MaxLoad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only lifted approaches\n",
    "data = data.loc[data['Lifted'] == 1]\n",
    "\n",
    "# Variables that aren't needed in the first run\n",
    "to_drop = [\n",
    "    'Id', 'Age', 'Height', 'Weight', 'PastInjuries', 'LastInjury', 'PainDuringTraining', 'SquatRecord',\n",
    "    'BenchPressRecord', 'DeadliftRecord', 'PhysicalActivities', 'SetNumber', 'Load', 'Lifted']\n",
    "\n",
    "data = data.drop(columns=to_drop)\n",
    "\n",
    "# Categorical variables that need to be one hot encoded\n",
    "to_one_hot = [\n",
    "    'ProficiencyLevel', 'EquipmentAvailability', 'TrainingProgram', 'TrainingFrequency', 'CameraPosition']\n",
    "\n",
    "dataframe = pd.get_dummies(data, columns=to_one_hot, dtype=int)\n",
    "\n",
    "# Move the PercentageMaxLoad column to the end of the dataframe\n",
    "percentage = dataframe.pop('PercentageMaxLoad')\n",
    "dataframe['PercentageMaxLoad'] = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.to_csv('test_dataframe.csv', index=False)\n",
    "\n",
    "# dataframe = pd.read_csv('test_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique file IDs\n",
    "FileIds = dataframe['FileId'].unique()\n",
    "\n",
    "# Split the files into three lists in an 8:1:1 ratio\n",
    "train, to_split = train_test_split(FileIds, test_size=0.2, random_state=42)\n",
    "\n",
    "valid, test = train_test_split(to_split, test_size=0.5, random_state=42)\n",
    "\n",
    "# \n",
    "train_data = dataframe.loc[dataframe['FileId'].isin(train)]\n",
    "valid_data = dataframe.loc[dataframe['FileId'].isin(valid)]\n",
    "test_data = dataframe.loc[dataframe['FileId'].isin(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_assign(data, batch_size, n_groups, ascending=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    data = data.sort_values(by='Frequency', ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "    data['GroupNumber'] = pd.cut(\n",
    "        data.index + 1,\n",
    "        bins = range(0, len(data) + batch_size, batch_size),\n",
    "        labels = range(n_groups)\n",
    "    )\n",
    "\n",
    "    tmp = data.groupby(by='GroupNumber', as_index=False)['Frequency'].max()\n",
    "    tmp = tmp.rename(columns={'Frequency': 'MaxFrequency'})\n",
    "\n",
    "    data = pd.merge(data, tmp, on='GroupNumber')\n",
    "\n",
    "    # Calculate how many frames should be added to each file on average\n",
    "    mean = (data['MaxFrequency'] - data['Frequency']).mean()\n",
    "    \n",
    "    return data, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_groups(data, batch_size):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a list of file IDs\n",
    "    file_ids = data['FileId'].unique()\n",
    "    # Calculate the number of groups\n",
    "    n_groups = int(np.ceil(len(file_ids) / batch_size))\n",
    "\n",
    "    #\n",
    "    freq_data = data.groupby(by='FileId', as_index=False).size()\n",
    "    freq_data = freq_data.rename(columns={'size': 'Frequency'})\n",
    "    \n",
    "    df_1, mean_1 = sort_and_assign(freq_data, batch_size, n_groups)\n",
    "    df_2, mean_2 = sort_and_assign(freq_data, batch_size, n_groups, ascending=False)\n",
    "\n",
    "    # Choose a better sorting option\n",
    "    if mean_1 > mean_2:\n",
    "        freq_data = df_2\n",
    "        # mean = mean_2\n",
    "    else:\n",
    "        freq_data = df_1\n",
    "        # mean = mean_1\n",
    "\n",
    "    return pd.merge(data, freq_data, on='FileId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor_ceil(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return int(np.floor(x)), int(np.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_dataframe(data):\n",
    "    \"\"\"\n",
    "    Adjust dataframe to the group max frequency\n",
    "    \"\"\"\n",
    "    # Reset index\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # Calculate how many records are missing to the maximum frequency\n",
    "    difference = data.loc[0, 'MaxFrequency'] - data.loc[0, 'Frequency']\n",
    "\n",
    "    if difference > 1:\n",
    "        # Calculate how many records should be added to the beginning and to the end\n",
    "        front, back = floor_ceil(difference / 2)\n",
    "\n",
    "        # Get the first and last record\n",
    "        first_record, last_record = data.iloc[0], data.iloc[-1]\n",
    "\n",
    "        # Prepare data frames\n",
    "        to_beginning = pd.concat(front * [pd.DataFrame([first_record])])\n",
    "        to_end = pd.concat(back * [pd.DataFrame([last_record])])\n",
    "\n",
    "        # Return concatenated data frames\n",
    "        return pd.concat([to_beginning, data, to_end], ignore_index=True)\n",
    "\n",
    "    elif difference == 1:\n",
    "        # Get only the last record\n",
    "        last_record = data.iloc[-1]\n",
    "\n",
    "        # Return concatenated data frames\n",
    "        return pd.concat([data, pd.DataFrame([last_record])], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 79])\n",
      "torch.Size([64, 75, 79])\n",
      "torch.Size([64, 84, 79])\n",
      "torch.Size([64, 90, 79])\n",
      "torch.Size([64, 96, 79])\n",
      "torch.Size([64, 105, 79])\n",
      "torch.Size([64, 111, 79])\n",
      "torch.Size([64, 120, 79])\n",
      "torch.Size([64, 124, 79])\n",
      "torch.Size([64, 132, 79])\n",
      "torch.Size([64, 141, 79])\n",
      "torch.Size([64, 150, 79])\n",
      "torch.Size([64, 162, 79])\n",
      "torch.Size([64, 180, 79])\n",
      "torch.Size([64, 210, 79])\n",
      "torch.Size([40, 238, 79])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "data = assign_groups(train_data, batch_size)\n",
    "\n",
    "for _, group_data in data.groupby(by='GroupNumber'):\n",
    "    # Drop the GroupNumber column\n",
    "    group_data = group_data.drop(columns='GroupNumber')\n",
    "\n",
    "    # Prepare group tensor storage\n",
    "    group_tensors = torch.tensor([])\n",
    "\n",
    "    for _, file_data in group_data.groupby(by='FileId'):\n",
    "        # Drop the FileId column\n",
    "        file_data = file_data.drop(columns='FileId')\n",
    "\n",
    "        # Adjust dataframe to MaxFrequency in group\n",
    "        adjusted = adjust_dataframe(file_data)\n",
    "        # Pick columns to drop\n",
    "        to_drop = ['Timestamp', 'Frequency', 'MaxFrequency']\n",
    "        # Drop unnecessary columns and convert the dataframe to a numpy array\n",
    "        array = adjusted.drop(columns=to_drop).to_numpy()\n",
    "\n",
    "        # Convert numpy array to pytorch tensor\n",
    "        tensor = torch.from_numpy(array).unsqueeze(dim=0)\n",
    "        # Concatenate to other tensors in the group\n",
    "        group_tensors = torch.cat((group_tensors, tensor), dim=0)\n",
    "    \n",
    "    print(group_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby(by='GroupNumber', as_index=False)\n",
    "\n",
    "for _, group in grouped_data:\n",
    "    \n",
    "    print(group)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Id')\n",
    "\n",
    "# Inicjalizacja listy na tensory\n",
    "tensory = []\n",
    "\n",
    "# Znalezienie maksymalnej długości grupy\n",
    "max_length = grouped.size().max()\n",
    "\n",
    "# Iteracja przez grupy i przekształcenie ich do tensorów\n",
    "for _, group in grouped:\n",
    "    # Usunięcie kolumny 'Id'\n",
    "    group = group.drop(columns=['Id'])\n",
    "    \n",
    "    # Konwersja DataFrame na tablicę numpy\n",
    "    array = group.to_numpy()\n",
    "    \n",
    "    # Przekształcenie tablicy numpy na tensor\n",
    "    tensor = np.zeros((1, max_length, len(group.columns)))\n",
    "    tensor[:, :len(group), :] = array.reshape(1, len(group), len(group.columns))\n",
    "    \n",
    "    # Dodanie tensora do listy\n",
    "    tensory.append(tensor)\n",
    "\n",
    "# Konkatenacja wszystkich tensorów wzdłuż pierwszej osi\n",
    "final_tensor = np.concatenate(tensory, axis=0)\n",
    "\n",
    "# Wyświetlenie finalnego tensora\n",
    "print(final_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderCustom(Dataset):\n",
    "    def __init__(self, target_dir, transform=None):\n",
    "        \n",
    "        self.data = pd.read_csv(target_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        # self.paths = list(pathlib.Path(target_dir).glob(\"*/*.jpg\"))\n",
    "        # self.classes, self.class_to_idx = find_classes(target_dir)\n",
    "\n",
    "    # def load_image(self, index):\n",
    "    #     image_path = self.paths[index]\n",
    "    #     return Image.open(image_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.groupby(by='FileId'))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video = self.data.loc[self.data['FileId'] == index]\n",
    "\n",
    "        img = self.load_image(index)\n",
    "        class_name = self.paths[index].parent.name\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx\n",
    "        else:\n",
    "            return img, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.  0.1 0.5 0.9]\n",
      "  [2.  0.2 0.6 1. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[3.  0.3 0.7 1.1]\n",
      "  [4.  0.4 0.8 1.2]\n",
      "  [5.  0.5 0.9 1.3]]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tworzenie przykładowego DataFrame'a z kolumną 'Id'\n",
    "data = {\n",
    "    'Id': [1, 1, 2, 2, 2],\n",
    "    'Timestamp': [1, 2, 3, 4, 5],\n",
    "    'RightFootIndexX': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'RightFootIndexY': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'RightFootIndexZ': [0.9, 1.0, 1.1, 1.2, 1.3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Grupowanie danych według wartości w kolumnie 'Id'\n",
    "grouped = df.groupby('Id')\n",
    "\n",
    "# Inicjalizacja listy na tensory\n",
    "tensory = []\n",
    "\n",
    "# Znalezienie maksymalnej długości grupy\n",
    "max_length = grouped.size().max()\n",
    "\n",
    "# Iteracja przez grupy i przekształcenie ich do tensorów\n",
    "for _, group in grouped:\n",
    "    # Usunięcie kolumny 'Id'\n",
    "    group = group.drop(columns=['Id'])\n",
    "    \n",
    "    # Konwersja DataFrame na tablicę numpy\n",
    "    array = group.to_numpy()\n",
    "    \n",
    "    # Przekształcenie tablicy numpy na tensor\n",
    "    tensor = np.zeros((1, max_length, len(group.columns)))\n",
    "    tensor[:, :len(group), :] = array.reshape(1, len(group), len(group.columns))\n",
    "    \n",
    "    # Dodanie tensora do listy\n",
    "    tensory.append(tensor)\n",
    "\n",
    "# Konkatenacja wszystkich tensorów wzdłuż pierwszej osi\n",
    "final_tensor = np.concatenate(tensory, axis=0)\n",
    "\n",
    "# Wyświetlenie finalnego tensora\n",
    "print(final_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

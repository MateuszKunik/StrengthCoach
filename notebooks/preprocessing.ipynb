{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths\n",
    "data_path = '../data/'\n",
    "processed_data_path = os.path.join(data_path, 'processed')\n",
    "\n",
    "# Read personal data from excel\n",
    "personal_data = pd.read_excel(os.path.join(data_path, 'PersonalData.xlsx'))\n",
    "\n",
    "# Read the data that is the result of the converted videos\n",
    "data = pd.read_csv(os.path.join(processed_data_path, 'AllSquats.csv'))\n",
    "\n",
    "# Merge personal and video data\n",
    "data = pd.merge(data, personal_data, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum load that was passed\n",
    "max_load = data.loc[data['Lifted'] == 1, ['Id', 'Load']].groupby(by='Id', as_index=False).max()\n",
    "max_load = max_load.rename(columns={'Load': 'MaxLoad'})\n",
    "data = pd.merge(data, max_load, on='Id')\n",
    "\n",
    "# Calculate what percentage of the maximum load is the current load\n",
    "data['PercentageMaxLoad'] = data['Load'] / data['MaxLoad']\n",
    "\n",
    "del data['MaxLoad']\n",
    "\n",
    "\n",
    "# Get only lifted approaches\n",
    "data = data.loc[data['Lifted'] == 1]\n",
    "\n",
    "# Variables that aren't needed in the first run\n",
    "to_drop = [\n",
    "    'Id', 'Age', 'Height', 'Weight', 'PastInjuries', 'LastInjury', 'PainDuringTraining', 'SquatRecord',\n",
    "    'BenchPressRecord', 'DeadliftRecord', 'PhysicalActivities', 'SetNumber', 'Load', 'Lifted', 'Timestamp']\n",
    "\n",
    "data = data.drop(columns=to_drop)\n",
    "\n",
    "# Categorical variables that need to be one hot encoded\n",
    "to_one_hot = [\n",
    "    'ProficiencyLevel', 'EquipmentAvailability', 'TrainingProgram', 'TrainingFrequency', 'CameraPosition']\n",
    "\n",
    "dataframe = pd.get_dummies(data, columns=to_one_hot, dtype=int)\n",
    "\n",
    "# Move the PercentageMaxLoad column to the end of the dataframe\n",
    "percentage = dataframe.pop('PercentageMaxLoad')\n",
    "dataframe['PercentageMaxLoad'] = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique file IDs\n",
    "file_ids = dataframe['FileId'].unique()\n",
    "\n",
    "# Split the files into three lists in an 8:1:1 ratio\n",
    "train_ids, ids_to_split = train_test_split(file_ids, test_size=0.2)\n",
    "\n",
    "valid_ids, test_ids = train_test_split(ids_to_split, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __call__(self, tensor):\n",
    "        #\n",
    "        min_value = tensor.min()\n",
    "        max_value = tensor.max()\n",
    "\n",
    "        # Normalization procedure\n",
    "        normalized = 2 * (tensor - min_value) / (max_value - min_value) - 1\n",
    "\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 1\n",
    "pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    dataframe.loc[dataframe['FileId'].isin(train_ids)],\n",
    "    transform=Norm()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    dataframe.loc[dataframe['FileId'].isin(valid_ids)],\n",
    "    transform=Norm()\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    dataframe.loc[dataframe['FileId'].isin(test_ids)],\n",
    "    transform=Norm()\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, device='cuda', dtype=torch.float64):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        # Initialize \n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize input to hidden state layer\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size, dtype=dtype)\n",
    "        # Initialize hidden state to output layer\n",
    "        self.fc = nn.Linear(hidden_size, 1, dtype=dtype)\n",
    "\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # Concatenate input and hidden tensors\n",
    "        combined = torch.cat((input_tensor.unsqueeze(dim=0), hidden_tensor), 1)\n",
    "\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.fc(hidden)\n",
    "\n",
    "        return hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 238, 78])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader))[0]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 78\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float64\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CustomRNN(input_size=78, hidden_size=128)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0, loss: 0.13243898084199185\n",
      "batch: 1, loss: 0.14506942547706375\n",
      "batch: 2, loss: 0.09656047494274622\n",
      "batch: 3, loss: 0.07118154356547206\n",
      "batch: 4, loss: 0.05537154544152028\n",
      "batch: 5, loss: 0.09846964176617123\n",
      "batch: 6, loss: 0.09731892511120008\n",
      "batch: 7, loss: 0.07409425919017369\n",
      "batch: 8, loss: 0.09112772576331318\n",
      "batch: 9, loss: 0.06318715488308507\n",
      "batch: 10, loss: 0.05450714218489702\n",
      "batch: 11, loss: 0.07325507025877741\n",
      "batch: 12, loss: 0.12059554791427138\n",
      "batch: 13, loss: 0.1981998602999847\n",
      "batch: 14, loss: 0.11003481937002285\n",
      "batch: 15, loss: 0.0599297699780742\n",
      "batch: 16, loss: 0.07371528910697367\n",
      "batch: 17, loss: 0.043689598251130994\n",
      "batch: 18, loss: 0.10735060160091554\n",
      "batch: 19, loss: 0.1328117789806159\n",
      "batch: 20, loss: 0.06740839998347338\n",
      "batch: 21, loss: 0.051677960514256625\n",
      "batch: 22, loss: 0.05263675199993149\n",
      "batch: 23, loss: 0.07053165445901295\n",
      "batch: 24, loss: 0.07229346493866429\n",
      "batch: 25, loss: 0.04623695908584583\n",
      "batch: 26, loss: 0.07151154544336578\n",
      "batch: 27, loss: 0.03702745118757423\n",
      "batch: 28, loss: 0.06793264646706369\n",
      "batch: 29, loss: 0.05606307649107954\n",
      "batch: 30, loss: 0.07099988328348841\n",
      "batch: 31, loss: 0.10400054765849387\n",
      "batch: 32, loss: 0.0716874760924895\n",
      "batch: 33, loss: 0.027860258454813245\n",
      "batch: 34, loss: 0.03150450238430526\n",
      "batch: 35, loss: 0.07920559144992098\n",
      "batch: 36, loss: 0.08013691082824997\n",
      "batch: 37, loss: 0.057371538618769966\n",
      "batch: 38, loss: 0.10928058573186736\n",
      "batch: 39, loss: 0.04248807754889844\n",
      "batch: 40, loss: 0.049543144785070825\n",
      "batch: 41, loss: 0.0753427912165399\n",
      "batch: 42, loss: 0.06069853339853071\n",
      "batch: 43, loss: 0.0773142738012681\n",
      "batch: 44, loss: 0.08882634514414202\n",
      "batch: 45, loss: 0.06827683215658278\n",
      "batch: 46, loss: 0.04396789096111228\n",
      "batch: 47, loss: 0.038676390110267654\n",
      "batch: 48, loss: 0.04382120686549242\n",
      "batch: 49, loss: 0.04004860664440098\n",
      "batch: 50, loss: 0.02291781528007146\n",
      "batch: 51, loss: 0.03735673606235343\n",
      "batch: 52, loss: 0.07225206543167378\n",
      "batch: 53, loss: 0.039311838178571284\n",
      "batch: 54, loss: 0.09301226054027906\n",
      "batch: 55, loss: 0.09315675348753356\n",
      "batch: 56, loss: 0.06117263168716973\n",
      "batch: 57, loss: 0.041441628915375325\n",
      "batch: 58, loss: 0.04131067326784304\n",
      "batch: 59, loss: 0.05788289817243674\n",
      "batch: 60, loss: 0.05605762912493543\n",
      "batch: 61, loss: 0.06492537382330206\n",
      "batch: 62, loss: 0.043764846414008074\n"
     ]
    }
   ],
   "source": [
    "# Put model in train mode\n",
    "model.train()\n",
    "\n",
    "# \n",
    "accumulated_loss = 0.\n",
    "\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # Send data to target device\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    # Prepare predictions storage\n",
    "    predictions = torch.tensor([], device=device)\n",
    "\n",
    "    for sample in data:\n",
    "        # Initialize hidden state tensor \n",
    "        hidden_tensor = model.init_hidden_state()\n",
    "\n",
    "        for frame in sample:\n",
    "            hidden_tensor, output = model(frame, hidden_tensor)\n",
    "        \n",
    "        # Concatenate all predictions per batch\n",
    "        predictions = torch.cat((predictions, output), dim=0)\n",
    "\n",
    "    # Calculate and accumulate loss\n",
    "    loss = loss_fn(targets, predictions)\n",
    "    accumulated_loss += loss.item()\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"batch: {batch_idx}, loss: {loss}\")\n",
    "\n",
    "# Get average loss per batch\n",
    "accumulated_loss = accumulated_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07107688258760203"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
